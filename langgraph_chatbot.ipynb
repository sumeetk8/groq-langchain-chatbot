{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOiAhtBgecZaDcVUigqYJuH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumeetk8/groq-langchain-chatbot/blob/master/langgraph_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU6QBTk-Wysq",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install necessary packages\n",
        "!pip install langgraph langsmith\n",
        "!pip install langchain langchain_groq langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import userdata from Colab to retrieve API keys\n",
        "from google.colab import userdata\n",
        "\n",
        "groq_api_key = userdata.get('groq_api_key')  # Retrieve the Groq API key\n",
        "langsmith_api_key = userdata.get('langsmith_api_key')  # Retrieve the LangSmith API key"
      ],
      "metadata": {
        "id": "xJL8j3FdXwy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment variable setup for LangSmith tracing and project identification\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"CourseLanggraph\""
      ],
      "metadata": {
        "id": "C9-OzxYnY95q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ChatGroq for language model interaction\n",
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "aRVzCMT5aQWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ChatGroq LLM with the specified model and API key\n",
        "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"gemma2-9b-it\")"
      ],
      "metadata": {
        "id": "Jl7Zs5KdaVzj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary types for StateGraph\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages"
      ],
      "metadata": {
        "id": "Ebtn3dq_auAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the state structure for the chatbot using TypedDict\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]  # Annotated to specify a list of messages"
      ],
      "metadata": {
        "id": "V23RLYLBbQ9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of StateGraph for managing state transitions\n",
        "graph_builder = StateGraph(State)"
      ],
      "metadata": {
        "id": "5KbAPcehivkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chatbot function that invokes the LLM based on the current state\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": llm.invoke(state['messages'])}  # LLM processes the input messages"
      ],
      "metadata": {
        "id": "NiIXjT95dC_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the chatbot function as a node to the state graph\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ],
      "metadata": {
        "id": "1nZf9w_LdUbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add edges to the state graph defining the start and end points\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)"
      ],
      "metadata": {
        "id": "Oi8JqOChdfb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the state graph for execution\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "N-9AuaW3dqcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Display the state graph as an image if possible\n",
        "from IPython.display import Image, display\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    pass  # If an error occurs, skip image display"
      ],
      "metadata": {
        "id": "rPaPSFn1dwJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive loop for the chatbot to receive user input and generate responses\n",
        "while True:\n",
        "    user_input = input(\"User: \")  # Prompt the user for input\n",
        "    if user_input.lower() in [\"quit\", \"q\"]:  # Check for quit command\n",
        "        print(\"Good Bye\")\n",
        "        break\n",
        "\n",
        "    # Stream responses from the graph based on the input message\n",
        "    for event in graph.stream({'messages': [(\"user\", user_input)]}):\n",
        "        print(\"Event:\", event)  # Print the raw event for debugging\n",
        "        print(\"Event Values:\", event.values())  # Print event values to inspect contents\n",
        "\n",
        "        if event.values() is None:\n",
        "            print(\"No values in event.\")  # Handle cases where event returns None\n",
        "        else:\n",
        "            for value in event.values():\n",
        "                if value is not None:\n",
        "                    print(value['messages'])  # Print the full message content\n",
        "                    print(\"Assistant:\", value[\"messages\"].content)  # Print the assistant's response\n",
        "                else:\n",
        "                    print(\"Received NoneType value\")  # Handle NoneType values explicitly"
      ],
      "metadata": {
        "id": "dR41ZIrfd5Rk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}